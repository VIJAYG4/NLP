{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re,collections\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real = pd.read_csv('/home/vijay/Documents/NLP/hw2/True.csv')\n",
    "df_real['RealNews?'] = True\n",
    "df_fake = pd.read_csv('/home/vijay/Documents/NLP/hw2/Fake.csv')\n",
    "df_fake['RealNews?'] = False\n",
    "df = df_real.append(df_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'text', 'subject', 'date', 'RealNews?'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['document']=df[['title','text']].agg(' '.join,axis=1).apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting data into train and test and creating separate dataframes for True and Fake News for training purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "df_train_true=df_train[df_train['RealNews?']]\n",
    "df_train_fake=df_train[~df_train['RealNews?']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate sizes of true and fake dataframes and probabilities of Real and Fake News."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Calculate Naive Bayes by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17216 18702 35918\n"
     ]
    }
   ],
   "source": [
    "true_count = len(df_train_true)\n",
    "fake_count = len(df_train_fake)\n",
    "prob_true = true_count/len(df_train)\n",
    "prob_fake = fake_count/len(df_train)\n",
    "print(true_count,fake_count,len(df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate count of each word in entire True and Fake dataframes respectively and calculating total number of words in true and fake dataframes and also calculating the size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "true,fake=collections.defaultdict(int),collections.defaultdict(int)\n",
    "# news={'true_words':{},'fake_words':{}}\n",
    "vocabulary_set=set()\n",
    "\n",
    "# for true documents\n",
    "for d in df_train_true['document']:\n",
    "    lst=re.split(r\"\\W+\",d)\n",
    "    for word in lst:\n",
    "        true[word]+=1\n",
    "        vocabulary_set.add(word)\n",
    "# news['true_words']=true\n",
    "\n",
    "#for fake documents\n",
    "for d in df_train_fake['document']:\n",
    "    lst=re.split(r\"\\W+\",d)\n",
    "    for word in lst:\n",
    "        fake[word]+=1\n",
    "        vocabulary_set.add(word)\n",
    "# news['fake_words']=fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6985363 8471575 112119\n"
     ]
    }
   ],
   "source": [
    "true_words_count=sum(true.values())\n",
    "fake_words_count=sum(fake.values())\n",
    "vocabulary_count=len(vocabulary_set)\n",
    "print(true_words_count,fake_words_count,vocabulary_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary to store conditional probabilities of each word according to true or fake news class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cond_prob_words=collections.defaultdict()\n",
    "fake_cond_prob_words=collections.defaultdict()\n",
    "\n",
    "for word in true:\n",
    "    true_cond_prob_words[word]=(true[word]+1)/(true_words_count+vocabulary_count)\n",
    "\n",
    "for word in fake:\n",
    "    fake_cond_prob_words[word]=(fake[word]+1)/(fake_words_count+vocabulary_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification on test dataframe and finding precision, recall and F1 score for our classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "test_result=[]\n",
    "for d in df_test['document']:\n",
    "#     print(d,end=\" \")\n",
    "    res_true,res_fake=log(prob_true),log(prob_fake)\n",
    "    for word in re.split(r\"\\W+\",d):\n",
    "        if word in true_cond_prob_words:\n",
    "            res_true += log(true_cond_prob_words[word])\n",
    "        else:\n",
    "            res_true += log((1/(true_words_count+vocabulary_count)))\n",
    "        \n",
    "        if word in fake_cond_prob_words:\n",
    "            res_fake += log(fake_cond_prob_words[word])\n",
    "        else:\n",
    "            res_fake += log((1/(fake_words_count+vocabulary_count)))\n",
    "    \n",
    "#     print(res_true,res_fake)\n",
    "    if res_true < res_fake:test_result.append(False)\n",
    "    else:test_result.append(True)\n",
    "        \n",
    "df_test['test_result']=test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.96334746, 0.94553991]),\n",
       " array([0.95145428, 0.95881933]),\n",
       " array([0.95736393, 0.95213332]),\n",
       " array([4779, 4201]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(df_test['RealNews?'], df_test['test_result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Calculate tfidf with logistic regression by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents=''\n",
    "for document in df_train['document']:\n",
    "    train_documents+=document+' '\n",
    "\n",
    "#remove the last space\n",
    "train_documents = train_documents[:-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112119\n"
     ]
    }
   ],
   "source": [
    "#tokenize the train docs\n",
    "train_docs_words_list = re.split(r\"\\W+\", train_documents)\n",
    "\n",
    "#count the words and its frequency\n",
    "train_words_freq = Counter(train_docs_words_list)\n",
    "#print(train_words_freq)\n",
    "print(len(train_words_freq))\n",
    "\n",
    "#keep only words whose freq is atleast two - for simpler processing\n",
    "#dict comprehension\n",
    "train_words_freq = { k:v for k, v in train_words_freq.items() if v>1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'll', 'put', 'a', 'bullet']\n"
     ]
    }
   ],
   "source": [
    "#training set vocab\n",
    "# V is list of all unique words in training set\n",
    "V = list(train_words_freq.keys())\n",
    "#print(V[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate tf-idf\n",
    "#calculate idf\n",
    "idf = {}\n",
    "N = len(df_train)\n",
    "for word in V:\n",
    "    cnt=0\n",
    "    for document in df_train['document']:\n",
    "        words_list = re.split(r\"\\W+\", document)\n",
    "        if word in words_list:\n",
    "            cnt+=1\n",
    "\n",
    "    df_word=cnt\n",
    "        #store the idf values of words dict\n",
    "    idf[word]=np.log(N/df_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mat = []\n",
    "#calculate tf\n",
    "for document in df_train['document']:\n",
    "    tfidf=[]\n",
    "    words_list = re.split(r\"\\W+\", document)\n",
    "    words_freq_map = Counter(words_list)\n",
    "    for word in V:\n",
    "        if word in words_freq_map:\n",
    "           word_freq = words_freq_map[word]\n",
    "           #calc tf of word\n",
    "           tf_word = 1+np.log(word_freq)\n",
    "           #calc tfidf of word\n",
    "           tfidf_word = tf_word*idf[word]\n",
    "           tfidf.append(tfidf_word)\n",
    "        else:\n",
    "           tfidf.append(0)\n",
    "\n",
    "    tfidf_mat.append(tfidf)\n",
    "\n",
    "print(tfidf_mat[:5,:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the logistic reg classifier\n",
    "X_train = tfidf_mat\n",
    "y_train = df_train['RealNews?']\n",
    "y_test = df_test['RealNews?']\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf for test data\n",
    "#calculate tf\n",
    "y_pred = []\n",
    "for document in df_test['document']:\n",
    "    tfidf=[]\n",
    "    words_list = re.split(r\"\\W+\", document)\n",
    "    words_freq_map = Counter(words_list)\n",
    "    for word in V:\n",
    "        if word in words_freq_map:\n",
    "           word_freq = words_freq_map[word]\n",
    "           #calc tf of word\n",
    "           tf_word = 1+np.log(word_freq)\n",
    "           #calc tfidf of word\n",
    "           tfidf_word = tf_word*idf[word]\n",
    "           tfidf.append(tfidf_word)\n",
    "        else:\n",
    "           tfidf.append(0)\n",
    "\n",
    "    X_test = tfidf\n",
    "    #predict \n",
    "    y_pred.append(clf.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate performance of the model\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "print(precision_recall_fscore_support(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part -3: Do Naive Bayes and tfidf-logistic reg using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorize the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(df_train['document'])\n",
    "y_train = df_train['RealNews?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35918, 112079)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorize the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8980, 112079)\n"
     ]
    }
   ],
   "source": [
    "X_test = vectorizer.transform(df_test['document'])\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build multinomial naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "#train\n",
    "mnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preidct on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False  True  True False]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.96159527, 0.94765386]),\n",
       " array([0.95354677, 0.95667698]),\n",
       " array([0.95755411, 0.95214404]),\n",
       " array([4779, 4201]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "y_test = df_test['RealNews?']\n",
    "precision_recall_fscore_support(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tf-idf with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "vectorizer_word = TfidfVectorizer(lowercase=True, analyzer='word',\n",
    "                        stop_words= 'english',dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35918, 111767)\n",
      "(8980, 111767)\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer_word.fit_transform(df_train['document'])\n",
    "X_test = vectorizer_word.transform(df_test['document'])\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['RealNews?']\n",
    "y_test = df_test['RealNews?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.99134291, 0.98020735]),\n",
       " array([0.9824231 , 0.99024042]),\n",
       " array([0.98686285, 0.98519834]),\n",
       " array([4779, 4201]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "precision_recall_fscore_support(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with one of the parameters of tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using character ngram(changing analyzer parameter from 'word' to 'char') - tokenizes based on characters instead of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_ch = TfidfVectorizer(lowercase=True, analyzer='char',\n",
    "                        stop_words= 'english',dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35918, 118)\n",
      "(8980, 118)\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer_ch.fit_transform(df_train['document'])\n",
    "X_test = vectorizer_ch.transform(df_test['document'])\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['RealNews?']\n",
    "y_test = df_test['RealNews?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.89038174, 0.85924027]),\n",
       " array([0.87361373, 0.87764818]),\n",
       " array([0.88191804, 0.86834668]),\n",
       " array([4779, 4201]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "precision_recall_fscore_support(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Changing the analyzer parameter from 'word' to 'char' brought down the precision, recall and f1 scores on the test data. Maybe try combination of 'word' and 'char' to get better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
